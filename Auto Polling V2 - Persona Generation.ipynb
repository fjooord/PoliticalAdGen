{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e20385",
   "metadata": {},
   "source": [
    "# Auto Polling\n",
    "    - The original persona generation worked, but the personas were too generic and didnt give a model context to \n",
    "        make accurate predictions of the beliefs of a person\n",
    "    - Now making a new pipeline that will create personas in different stages to \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f276369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils.Chat_GPT_Funcs as GPT\n",
    "import concurrent.futures\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346d525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5973b7",
   "metadata": {},
   "source": [
    "# Load First and Last Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f34d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eths = [\"White\", \"American Indian or Alaska Native\", \"Black or African American\", \"Asian\", \"Hispanic or Latino\"]\n",
    "genders = ['Men', 'Women']\n",
    "\n",
    "dems = []\n",
    "for e in eths:\n",
    "    for g in genders:\n",
    "        s = f\"{e} {g}\"\n",
    "        dems.append(s)\n",
    "        \n",
    "first_names = {}\n",
    "for d in dems:\n",
    "    # Open the file in read binary mode\n",
    "    with open(f\"Names/First_Names/{d}.pkl\", \"rb\") as file:\n",
    "        # Use pickle to load the list object from the file\n",
    "        my_list = pickle.load(file)\n",
    "        first_names[d] = my_list\n",
    "\n",
    "last_names = {}\n",
    "for e in eths:\n",
    "    # Open the file in read binary mode\n",
    "    with open(f\"Names/Last_Names/{e}.pkl\", \"rb\") as file:\n",
    "        # Use pickle to load the list object from the file\n",
    "        my_list = pickle.load(file)\n",
    "        last_names[e] = my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec575d3",
   "metadata": {},
   "source": [
    "# Distributing Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800610d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_geographic_location():\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.44:\n",
    "        return \"Rural Montana, United States\"\n",
    "    else:\n",
    "        return \"Urban Montana, United States\"\n",
    "    \n",
    "def distribute_gender():\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.49:\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"Male\"\n",
    "    \n",
    "def distribute_ethnicity():\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.88:\n",
    "        return \"White\"\n",
    "    elif random_number < 0.94:\n",
    "        return \"American Indian or Alaska Native\"\n",
    "    elif random_number < 0.95:\n",
    "        return \"Black or African American\"\n",
    "    elif random_number < 0.96:\n",
    "        return \"Asian\"\n",
    "    else:\n",
    "        return \"Hispanic or Latino\"\n",
    "    \n",
    "def distribute_age_and_income():\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.13:\n",
    "        age = random.randint(18, 25)\n",
    "        \n",
    "        # Set the mean and standard deviation of the normal distribution\n",
    "        mean = 32413  # Mean of the distribution\n",
    "        std_dev = 10000  # Standard deviation of the distribution\n",
    "        income = round(int(np.random.normal(mean, std_dev, size=1)[0]), -2)\n",
    "        \n",
    "        return age, income\n",
    "    elif random_number < 0.44:\n",
    "        age = random.randint(25, 44)\n",
    "        \n",
    "        # Set the mean and standard deviation of the normal distribution\n",
    "        mean = 69192  # Mean of the distribution\n",
    "        std_dev = 20000  # Standard deviation of the distribution\n",
    "        income = round(int(np.random.normal(mean, std_dev, size=1)[0]), -2)\n",
    "        \n",
    "        return age, income\n",
    "    elif random_number < 0.75:\n",
    "        age = random.randint(45, 64)\n",
    "        \n",
    "        # Set the mean and standard deviation of the normal distribution\n",
    "        mean = 71064  # Mean of the distribution\n",
    "        std_dev = 25000  # Standard deviation of the distribution\n",
    "        income = round(int(np.random.normal(mean, std_dev, size=1)[0]), -2)\n",
    "        \n",
    "        return age, income\n",
    "    else:\n",
    "        age = random.randint(65, 87)\n",
    "        \n",
    "        # Set the mean and standard deviation of the normal distribution\n",
    "        mean = 47015  # Mean of the distribution\n",
    "        std_dev = 15000  # Standard deviation of the distribution\n",
    "        income = round(int(np.random.normal(mean, std_dev, size=1)[0]), -2)\n",
    "        \n",
    "        return age, income\n",
    "    \n",
    "def distribute_education_level():\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.60:\n",
    "        return \"High school graduate\"\n",
    "    elif random_number < 0.94:\n",
    "        return \"Bachelor's Degree or Higher\"\n",
    "    else:\n",
    "        return \"Less than a high school education\"\n",
    "    \n",
    "def distribute_party():\n",
    "    \"\"\"\n",
    "    # Distribute Politcal Affiliation\n",
    "    - Reps that say they are conservative at Con\n",
    "    - Reps say moderate, right leaning\n",
    "    - Anyone that contradicts or says dont know is moderate\n",
    "    - Dems that say moderate, left leaning\n",
    "    - Dems that say liberal, liberal\n",
    "    \"\"\"\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.42:\n",
    "        return \"Conservative\"\n",
    "    elif random_number < 0.56:\n",
    "        return \"Right Leaning\"\n",
    "    elif random_number < 0.66:\n",
    "        return \"Moderate\"\n",
    "    elif random_number < 0.81:\n",
    "        return \"Left Leaning\"\n",
    "    else:\n",
    "        return \"Liberal\"\n",
    "    \n",
    "def distribute_veteran():\n",
    "    random_number = random.random()\n",
    "    \n",
    "    if random_number < 0.9:\n",
    "        return \"Not a Veteran\"\n",
    "    else:\n",
    "        return \"Is a Veteran\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ffc9b",
   "metadata": {},
   "source": [
    "# Generate Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd65f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(eth, gend):\n",
    "    if gend == 'Male':\n",
    "        gen = 'Men'\n",
    "    else:\n",
    "        gen = 'Women'\n",
    "    f_names = first_names[f\"{eth} {gen}\"]\n",
    "    \n",
    "    first = f_names[random.randint(0,len(f_names)-1)].strip()\n",
    "    \n",
    "    l_names = last_names[f\"{eth}\"]\n",
    "    last = l_names[random.randint(0,len(l_names)-1)].strip()\n",
    "    \n",
    "    return f\"{first} {last}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99f3d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_persona():\n",
    "    p = {}\n",
    "    \n",
    "    # Distribute Ethnicity\n",
    "    eth = distribute_ethnicity()\n",
    "    # Distribute Gender\n",
    "    gend = distribute_gender()\n",
    "    \n",
    "    p['Name'] = generate_name(eth, gend)\n",
    "    \n",
    "    # Distribute Age and Income\n",
    "    age, income = distribute_age_and_income()\n",
    "    p['Age'] = str(age)\n",
    "    p['Ethnicity'] = eth\n",
    "    p['Gender'] = gend\n",
    "    \n",
    "    p['Income'] = str(income)\n",
    "\n",
    "    # Distribute Education Level\n",
    "    p['Education Level'] = distribute_education_level()\n",
    "    \n",
    "    # Distribute Political Affiliation\n",
    "    p['Political Affiliation'] = distribute_party()\n",
    "    \n",
    "    # Distribute Geographic Location\n",
    "    p['Geographic Location'] = distribute_geographic_location()\n",
    "    \n",
    "    # Distribute Veteran Status\n",
    "    p['Veteran Status'] = distribute_veteran()\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8260bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_size = 100\n",
    "pp = []\n",
    "for i in range(poll_size):\n",
    "    pp.append(make_persona())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83fd2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc79a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_occupation(p):\n",
    "    occ_gen_role = GPT.open_file(\"Prompts/OccupationGen/occ_gen_role.txt\")\n",
    "    prompt = GPT.open_file(\"Prompts/OccupationGen/occ_gen_prompt.txt\").replace('<<PERSON>>', str(p))\n",
    "    occ, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 50, role=occ_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Occupation'] = occ\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44069043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID afc783503026b6b45fd939ee0e3e28e9 in your message.)\n"
     ]
    }
   ],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_occ = list(executor.map(gen_person_occupation, pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd7006b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_media(p):\n",
    "    media_gen_role = GPT.open_file(\"Prompts/MediaGen/media_gen_role.txt\")\n",
    "    prompt = GPT.open_file(\"Prompts/MediaGen/media_gen_in.txt\").replace('<<PERSON>>', str(p))\n",
    "    media, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 400, role=media_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Media Consumption'] = media\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "190af8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a29ea600c4fe8593c5a61fddd2be3058 in your message.)\n",
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a2aa0649983bca9dd7a692742f46513c in your message.)\n"
     ]
    }
   ],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_media = list(executor.map(gen_person_media, pp_occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f81381a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_voting_history(p):\n",
    "    voting_gen_role = GPT.open_file(\"Prompts/AskingPersona/load_persona_role.txt\").replace('<<PERSON>>', str(p))\n",
    "    prompt = GPT.open_file(\"Prompts/AskingPersona/gen_voting_history.txt\")\n",
    "    vh, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 400, role=voting_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Voting History'] = vh\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "830d2cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 99340a5ec629c7e9f0cabaa22b3654b4 in your message.)\n"
     ]
    }
   ],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_vote = list(executor.map(gen_person_voting_history, pp_media))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1df773c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_group_membership(p):\n",
    "    voting_gen_role = GPT.open_file(\"Prompts/AskingPersona/load_persona_role.txt\").replace('<<PERSON>>', str(p))\n",
    "    prompt = GPT.open_file(\"Prompts/AskingPersona/gen_group_membership.txt\")\n",
    "    vh, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 400, role=voting_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Group Membership'] = vh\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a82e57c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1aa54845c6e104ae274f885c8ee9fb0e in your message.)\n",
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 287d48cd72effe31e566cb0cb82aa563 in your message.)\n"
     ]
    }
   ],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_mem = list(executor.map(gen_person_group_membership, pp_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c39ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_group_membership(p):\n",
    "    voting_gen_role = GPT.open_file(\"Prompts/AskingPersona/load_persona_role.txt\").replace('<<PERSON>>', str(p))\n",
    "    prompt = GPT.open_file(\"Prompts/AskingPersona/gen_influential_figures.txt\")\n",
    "    vh, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 400, role=voting_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Influential Figures'] = vh\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "754e0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID afe405028de22c6d47e7784299cd3253 in your message.)\n"
     ]
    }
   ],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_if = list(executor.map(gen_person_group_membership, pp_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6e5627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_personal_anecdotes(p):\n",
    "    voting_gen_role = GPT.open_file(\"Prompts/AskingPersona/load_persona_role.txt\").replace('<<PERSON>>', str(p))\n",
    "    prompt = GPT.open_file(\"Prompts/AskingPersona/personal_anecdotes.txt\")\n",
    "    vh, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 400, role=voting_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Personal Anecdotes'] = vh\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3411bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6d3e03cd0661f8796a096d966a88385 in your message.)\n",
      "Error communicating with OpenAI: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 19 May 2023 18:07:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7c9e4028f8fa81bb-IAD', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_anec = list(executor.map(gen_person_personal_anecdotes, pp_if))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da4eb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_person_future(p):\n",
    "    voting_gen_role = GPT.open_file(\"Prompts/AskingPersona/load_persona_role.txt\").replace('<<PERSON>>', str(p))\n",
    "    prompt = GPT.open_file(\"Prompts/AskingPersona/gen_future_aspirations_and_concerns.txt\")\n",
    "    vh, usage = GPT.chat_gpt(prompt, engine='gpt-3.5-turbo', temp=0.1, tokens = 400, role=voting_gen_role)\n",
    "    \n",
    "    global TOKENS\n",
    "    TOKENS += usage['total_tokens']\n",
    "    \n",
    "    p['Future Aspirations/Concerns'] = vh\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72897d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread pool with 10 workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit each person to the worker pool\n",
    "    pp_future = list(executor.map(gen_person_future, pp_anec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50100de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422011"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "879ad013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844022\n"
     ]
    }
   ],
   "source": [
    "cost = TOKENS/1000*0.002\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b11d0b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Kevin Evans',\n",
       " 'Age': '64',\n",
       " 'Ethnicity': 'White',\n",
       " 'Gender': 'Male',\n",
       " 'Income': '86900',\n",
       " 'Education Level': 'High school graduate',\n",
       " 'Political Affiliation': 'Right Leaning',\n",
       " 'Geographic Location': 'Urban Montana, United States',\n",
       " 'Veteran Status': 'Not a Veteran',\n",
       " 'Occupation': 'Retired Business Owner.',\n",
       " 'Media Consumption': 'Based on the demographic information provided, Kevin Evans is likely to trust and consume media outlets that cater to a right-leaning, older, white, male audience. He may also be interested in news and information related to business and finance.\\n\\nSome media outlets that Kevin Evans may trust and consume regularly include:\\n\\n- Fox News\\n- The Wall Street Journal\\n- The Drudge Report\\n- Breitbart News\\n- The Blaze\\n\\nAs a retired business owner, Kevin Evans may have more free time to consume media than someone who is still working. However, as an older individual, he may not be as tech-savvy as younger generations and may prefer traditional forms of media such as television and print. \\n\\nTherefore, he may consume around 3-4 hours of media per day, with a mix of television news, online news sources, and print newspapers.',\n",
       " 'Voting History': 'As a retired business owner, I have consistently voted in the past. I tend to lean towards the right-leaning political party and have voted for candidates who align with my beliefs and values. However, I also take the time to research and educate myself on the candidates and their policies before making a decision.',\n",
       " 'Group Membership': 'As a retired business owner, I am not currently part of any professional organizations. However, I do tend to lean towards the right-leaning political party and have voted for candidates who align with my beliefs and values. I also keep myself informed about current events and policies through various media outlets that cater to a right-leaning, older, white, male audience.',\n",
       " 'Influential Figures': 'As a retired business owner who tends to lean towards the right-leaning political party, I do have some politicians that I admire and respect. One of them is Ronald Reagan, who was the President of the United States from 1981 to 1989. I appreciate his conservative values and his efforts to reduce government spending and regulation. I also respect his foreign policy, particularly his stance against communism.',\n",
       " 'Personal Anecdotes': 'As a retired business owner, I have had my fair share of experiences that have shaped my political views. One of the most significant experiences was the economic hardship that I faced during the recession of 2008. It was a challenging time for me, and I saw firsthand how government policies and regulations can impact small businesses like mine. This experience made me appreciate the importance of limited government intervention in the economy and the need for policies that promote business growth and job creation.\\n\\nAdditionally, I have always been interested in foreign policy, and my views have been shaped by the events that have occurred during my lifetime. For example, I have a deep respect for Ronald Reagan and his foreign policy, particularly his stance against communism. I believe that a strong national defense is essential to protect our country and our interests abroad.\\n\\nOverall, my political views have been shaped by my personal experiences, my values, and my beliefs about the role of government in society.',\n",
       " 'Future Aspirations/Concerns': 'As a retired business owner, my hopes for the future are centered around the economy and the growth of small businesses. I hope to see policies that promote job creation and business growth, as I believe that a strong economy is essential for the well-being of the country and its citizens.\\n\\nHowever, I also have some fears for the future. One of my biggest concerns is the increasing polarization and divisiveness in our society. I worry that this will lead to a breakdown in civil discourse and a lack of cooperation between political parties, which could ultimately harm the country as a whole.\\n\\nAdditionally, I am concerned about the growing national debt and the impact that it could have on future generations. I believe that it is important for the government to address this issue and work towards a more fiscally responsible future.\\n\\nOverall, my hopes and fears for the future are centered around the economy, the well-being of the country, and the need for cooperation and responsible governance.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_future[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24132cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"new_personas_100.json\"\n",
    "\n",
    "# Open the file in write mode and save the dictionary as JSON\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(pp_future, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6eb47c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_paragraph(persona, residence):\n",
    "    priorities = ', '.join(persona[\"Top Priorities\"])\n",
    "    try:\n",
    "        pain_points = ', '.join(persona[\"Pain points\"])\n",
    "    except:\n",
    "        pain_points = ', '.join(persona[\"Pain Points\"])\n",
    "    \n",
    "    return (f\"Your name is {persona['Name']}, you are a {persona['Age']} year old {persona['Ethnicity']} {persona['Gender']}. \"\n",
    "            f\"You live in {residence} and your annual income is ${persona['Income']}. \"\n",
    "            f\"Your marital status is {persona['Marital Status']}, and your highest level of education is {persona['Education Level']}. \"\n",
    "            f\"You work as a {persona['Occupation']}. You are described as '{persona['Description']}'. \"\n",
    "            f\"Your top priorities are {priorities}. You are facing some challenges, including {pain_points}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8014467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
